{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction\n",
    "\n",
    "## Encodage\n",
    "\n",
    "- Utilisation du OneHotEncoding\n",
    "- Scaling de nos données continues\n",
    "- Passage au log pour les données skewées\n",
    "\n",
    "## Sélection\n",
    "\n",
    "# Sources d'apprentissage externes\n",
    "\n",
    "- [Etre plus à l'aise avec l'IA de manière générale](https://www.youtube.com/watch?v=P6kSc3qVph0&list=PLO_fdPEVlfKoHQ3Ua2NtDL4nmynQC8YiS)\n",
    "- Shap : [YT](https://www.youtube.com/watch?v=IqT551LjKHw) / [Kaggle](https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-shapley-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 100\n",
    "np.set_printoptions(linewidth=180)\n",
    "sns.set_style(\"dark\")\n",
    "sns.axes_style(\"darkgrid\")\n",
    "\n",
    "data = pd.read_csv(\"cleaned.csv\")\n",
    "data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itération 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df = df.drop(columns=\"ENERGYSTARScore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut déjà sélectionner les features à OneHotEncoder.\n",
    "\n",
    "On traitera les PropertyUseType séparemment puisqu'il y a un lien entre les 3 features UseTypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_types = [\"LargestPropertyUseType\", \"SecondLargestPropertyUseType\", \"ThirdLargestPropertyUseType\"]\n",
    "use_types_gfa = [x+\"GFA\" for x in use_types]\n",
    "unique_use_types = np.array(list(set(df[use_types].to_numpy().ravel())))\n",
    "unique_use_types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite avoir un dataframe avec en colonnes les valeurs possibles d'un use_type et en donnée un 1 ou un 0 suivant si ce type existe dans LargestPropertyUseType, SecondLargestPropertyUseType, ThirdLargestPropertyUseType.\n",
    "\n",
    "Mais on a un problème. On aura un tableau du style\n",
    "\n",
    "| LargestPropertyUseTypeGFA |SecondLargestPropertyUseTypeGFA |ThirdLargestPropertyUseTypeGFA | Adult Education | Automobile Dealership | Bank Branch | Bar/Nightclub | ... |\n",
    "|-|-|-|-|-|-|-| - |\n",
    "|0.45|0.10|0.05|0|1|1|0| ... |\n",
    "\n",
    "Comment dans notre example on peut retrouver que le LargestPropertyUseType de 0.45 fait référence à la Bank Branch ? Aucun moyen si on reste sur cette structure.\n",
    "\n",
    "La solution est de ne pas voir les données en OneHotEncode. Ainsi on aurait ce résultat :\n",
    "\n",
    "| Adult Education | Automobile Dealership | Bank Branch | Bar/Nightclub | ... |\n",
    "|-|-|-|-| - |\n",
    "|0|0.10|0.45|0| ... |\n",
    "\n",
    "Ici on conserve l'information du GFA mais on la met directement dans la colonne représentant la valeur elle même. Ainsi le modèle pourra pondérer en fonction du bon UseType.\n",
    "\n",
    "Cela nous permet également de retirer nos features de GFA ce qui réduit la dimension d'entrée au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_use_types(df, use_types_gfa=use_types_gfa):\n",
    "    use_types_gfa_df = pd.DataFrame()\n",
    "\n",
    "    df[use_types_gfa] = df[use_types_gfa].replace(np.nan, 0)\n",
    "\n",
    "    x_max = df[use_types_gfa].values.max()\n",
    "    x_min = df[use_types_gfa].values.min()\n",
    "\n",
    "    for (i,line), gfa in zip(enumerate(df[use_types].values), df[use_types_gfa].values):\n",
    "        # Clés sans doublons\n",
    "        line_k = np.unique(np.array(list(filter(lambda x: x==x, line))))\n",
    "        # On met les valeurs de gfa pour ces clés\n",
    "        use_types_gfa_df.loc[i,[f\"{x}_ut\" for x in line_k]] = gfa[:len(line_k)]\n",
    "\n",
    "    use_types_gfa_df = (use_types_gfa_df - x_min) / (x_max - x_min)\n",
    "    \n",
    "    df = df.drop(columns=use_types+use_types_gfa)\n",
    "    df[use_types_gfa_df.columns] = use_types_gfa_df.replace(np.nan, 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "encode_use_types(df).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit faire attention à appliquer un scaling sur les données de GFA dans l'ensemble avant de faire cet encodage.\n",
    "\n",
    "En effet si on ne fait pas ça le scaling s'effectuera sur les UseTypes de façon indépendantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(data=df[[\"Latitude\", \"Longitude\", \"PropertyGFABuilding(s)\", \"PropertyGFAParking\", \"LargestPropertyUseTypeGFA\", \"SecondLargestPropertyUseTypeGFA\", \"ThirdLargestPropertyUseTypeGFA\"]], kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"TotalGHGEmissions\", \"SiteEnergyUse(kBtu)\"]\n",
    "ohe_cols = [\"BuildingType\", \"ZipCode\", \"CouncilDistrictCode\", \"Neighborhood\", \"YearBuiltRange\", \"Parking\"]\n",
    "std_numerical_cols = [\"Latitude\", \"Longitude\", \"PropertyGFABuilding(s)\", \"PropertyGFAParking\"] + targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def encoding(df, categorical_cols, std_cols, suffix=\"_std\"):\n",
    "    # Scaling\n",
    "    for col in std_cols:\n",
    "        df[f\"{col}{suffix}\"] = (df[col] - df[col].mean()) / df[col].std()\n",
    "        df = df.drop(columns=col)\n",
    "\n",
    "    # Use types processing\n",
    "    df = encode_use_types(df, use_types_gfa=use_types_gfa)\n",
    "\n",
    "    # OneHotEncoding\n",
    "    for col in categorical_cols:\n",
    "        dummies = pd.get_dummies(df[col]).replace(np.nan, 0)\n",
    "        df = pd.concat([df, dummies], axis=1).drop(columns=col)\n",
    "\n",
    "    return df\n",
    "\n",
    "encoding(df, ohe_cols, std_numerical_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magnifique !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def preprocessing(df, targets, categorical_cols, std_cols, random_state):\n",
    "    df = encoding(df, categorical_cols, std_cols)\n",
    "\n",
    "    targets = [f\"{x}_std\" for x in targets]\n",
    "\n",
    "    target_splits = []\n",
    "    X = df.drop(columns=targets)\n",
    "    names = np.array([str(x) for x in X.columns])\n",
    "    X = X.values\n",
    "    for target in targets:\n",
    "        y = df[target].values.ravel()\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "        print(f\"Target {target} with {X.shape[1]} features ({y.shape[0]} lines). Train={X_train.shape[0]} ; Test={X_test.shape[0]}\")\n",
    "\n",
    "        target_splits.append([X_train, X_test, y_train, y_test])\n",
    "\n",
    "    return target_splits, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "(emissionTTS, energyTTS), feature_names = preprocessing(df, targets, ohe_cols, std_numerical_cols, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = DummyRegressor()\n",
    "\n",
    "def evaluation(model, tts, cv=5, verbose=0):\n",
    "    X_train, X_test, y_train, y_test = tts\n",
    "\n",
    "    r2s = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"r2\")\n",
    "\n",
    "    if(verbose):\n",
    "        print(\" \"*7 + \" Validation set metrics\")\n",
    "        print(f\"{'R2': <6} : {r2s.mean():.4f}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    if(verbose):\n",
    "        print(\" \"*7 + \" Test set metrics\")\n",
    "        print(f\"{'RMSE': <6} : {mean_squared_error(y_test, y_pred, squared=False):.2f}\")\n",
    "        print(f\"{'MAE': <6} : {median_absolute_error(y_test, y_pred):.2f}\")\n",
    "        print(f\"{'R2': <6} : {test_r2:.4f}\")\n",
    "    return r2s, test_r2\n",
    "\n",
    "evaluation(model, emissionTTS, verbose=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    BaggingRegressor\n",
    ")\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "def compare_models(tts, cv=5):\n",
    "    models = [\n",
    "        SVR(C=500, epsilon=0.001, tol=1e-1),\n",
    "        # MLPRegressor((150,100, 80, 50), random_state=random_state, alpha=0.001, max_iter=500), # Using this for testing only but not in the scope of this project\n",
    "        RidgeCV(alphas=np.linspace(0.01, 1000, 50)),\n",
    "        RandomForestRegressor(random_state=random_state),\n",
    "        ElasticNetCV(random_state=random_state),\n",
    "        GradientBoostingRegressor(random_state=random_state),\n",
    "        DecisionTreeRegressor(random_state=random_state),\n",
    "        KNeighborsRegressor(),\n",
    "        BaggingRegressor(n_estimators=100),\n",
    "        LassoCV(tol=0.01, random_state=random_state)\n",
    "    ]\n",
    "\n",
    "    val_keys = []\n",
    "    if isinstance(cv, BaseCrossValidator):\n",
    "        print(\"ISTANCE\")\n",
    "        val_keys = [f\"val_r2_{i+1}\" for (i) in range(cv.get_n_splits())]\n",
    "    else:\n",
    "        val_keys = [f\"val_r2_{i+1}\" for (i) in range(cv)]\n",
    "    \n",
    "\n",
    "    metrics = pd.DataFrame(columns=[\"name\"] + val_keys + [\"val_r2\", \"test_r2\"])\n",
    "\n",
    "    for model in tqdm(models):\n",
    "        name = type(model).__name__\n",
    "        vals_r2, test_r2 = evaluation(model, tts, cv=cv)\n",
    "        metrics.loc[len(metrics)] = {\"name\": name} | {k:v for (k,v) in zip(val_keys, vals_r2)} | {\"val_r2\" : vals_r2.mean(), \"test_r2\": test_r2}\n",
    "\n",
    "    return metrics, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_metrics, emission_models = compare_models(emissionTTS)\n",
    "emission_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_metrics, energy_models = compare_models(energyTTS)\n",
    "energy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_perfs(target_vals):\n",
    "    _, axs = plt.subplots(1, 2, figsize=(16,4))\n",
    "    for i, v_type in enumerate([\"val_r2\", \"test_r2\"]):\n",
    "        sns.scatterplot(data=target_vals[0], x=\"name\", y=v_type, ax=axs[i])\n",
    "        sns.scatterplot(data=target_vals[1], x=\"name\", y=v_type, ax=axs[i])\n",
    "        axs[i].set_xticklabels(energy_metrics.name, rotation=45, ha=\"right\")\n",
    "        axs[i].set_title(f\"Models {v_type} on targets\")\n",
    "        axs[i].legend(targets)\n",
    "    plt.show()\n",
    "\n",
    "show_model_perfs([emission_metrics, energy_metrics])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globalement notre jeu de donnée est plus pertinent pour effectuer des prédictions sur TotalGHGEmissions. On remarque que les 2 targets sont inversées au niveau des résultats entre le validation set et le test test.\n",
    "\n",
    "- TotalGHGEmission est plus bas que SiteEnergyUse sur le validation set que sur le test set\n",
    "- SiteEnergyUse est plus bas que TotalGHGEmission sur le test set que sur le validation set\n",
    "\n",
    "Il y a certainement des variables moins pertinentes pour la prédiction de SiteEnergyUse(kBtu).\n",
    "\n",
    "Analysons les meilleurs modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "shap_models = [emission_models[1], emission_models[-1], emission_models[3], emission_models[4]]\n",
    "\n",
    "emissions = emissionTTS[0][:1000]\n",
    "\n",
    "for model in shap_models:\n",
    "    print(type(model).__name__)\n",
    "    evaluation(model, emissionTTS)\n",
    "    explainer = shap.Explainer(model.predict, masker=emissions, feature_names=feature_names)\n",
    "    shap_values = explainer(emissions)\n",
    "    \n",
    "    shap.plots.bar(shap_values, max_display=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la surface est bien majoritairement utilisée par nos modèles pour effectuer nos prédictions. Peu importe le modèle parmis les 3 présentés.\n",
    "\n",
    "Mais la feature la plus représentée est celle concernant les hôpitaux. Ainsi, si le bâtiment est un hopital, alors sa consommation sera très élevée.\n",
    "\n",
    "- RidgeCV conserve beaucoup d'autre features, ce qui est logique. RidgeCV Regroupe les features par similarité\n",
    "- LassoCV lui cherche à annuler les features qui ne l'interesse pas, c'est ce qu'on voit sur son analyse shap\n",
    "- L'ElasticNet lui est un entre 2 qui prends en compte les 2 régularisations l1 et l2 (Ridge et Lasso), ce que l'on peut voir sur l'analyse\n",
    "- GradientBoostingRegressor va incrémentalement chercher à optimiser un modèle\n",
    "\n",
    "D'après l'analyse on a peu de lignes avec des hopitaux mais celles qui existent ont des valeurs de GFA élevées. Or ce n'est pas la seule catégorie comme cela. Il y a aussi \"Courthouse\" et \"Convention Center\" par exemple.\n",
    "\n",
    "Nos premiers modèles peuvent donc être améliorés en sélectionnant / en regroupant les bonnes features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping best models\n",
    "target_models = [\n",
    "    (RidgeCV(alphas=np.linspace(0.01, 1000, 20), scoring='r2'), {\"cv\": np.arange(3, 10)}),\n",
    "    (LassoCV(alphas=np.linspace(0.01, 1000, 20), random_state=random_state), {\"tol\": np.linspace(1e-4, 1e-2, 4)}),\n",
    "    (ElasticNetCV(alphas=np.linspace(0.01, 1000, 20), random_state=random_state), {}),\n",
    "    (GradientBoostingRegressor(loss=\"squared_error\", random_state=random_state), {\"learning_rate\": np.linspace(1e-3, 1e-1, 4), \"max_depth\": np.linspace(3, 15, 4).astype(\"int\")})\n",
    "]\n",
    "\n",
    "target_tts_model = list(zip(targets, [emissionTTS, energyTTS], [target_models, target_models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, tts, models_params in target_tts_model:\n",
    "    print(\"=\"*5 + f\"> {target}\")\n",
    "    # show r2's\n",
    "    for model,_ in models_params:\n",
    "        print(type(model).__name__)\n",
    "        evaluation(model, tts, verbose=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut déjà effectuer un petit GridSearch pour essayer d'améliorer nos performances et voir s'il y a une différence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def search_cv(model, tts, params):\n",
    "    X_train, _, y_train, __ = tts\n",
    "\n",
    "    cv = GridSearchCV(\n",
    "        model,\n",
    "        cv=3,\n",
    "        param_grid=params,\n",
    "        scoring=[\"r2\", \"neg_root_mean_squared_error\", \"neg_median_absolute_error\"],\n",
    "        n_jobs=-1,\n",
    "        refit=\"r2\"\n",
    "    )\n",
    "    cv.fit(X_train, y_train)\n",
    "    return cv\n",
    "\n",
    "cvs = []\n",
    "\n",
    "for target, tts, model_params in target_tts_model:\n",
    "    print(\"GridSearchs on \" + target + \" :\")\n",
    "    for model, params in tqdm(model_params):\n",
    "        df_cv = search_cv(model, tts, params)\n",
    "        cvs.append(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalGHGEmissions - Ridge\n",
    "pd.DataFrame(cvs[0].cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalGHGEmissions - Lasso\n",
    "pd.DataFrame(cvs[1].cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalGHGEmissions - Elastic\n",
    "pd.DataFrame(cvs[2].cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalGHGEmissions - GradientBoosting\n",
    "pd.DataFrame(cvs[3].cv_results_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch - TotalGHGEmissions\n",
    "\n",
    "On remarque que les splits ont des valeurs de r2 très différentes. C'est certainement car il y a une présence de valeurs un peu grandes dans ceux-ci qui font que quand on passe sur le validation set nos modèle ont du mal à effectuer les prédictions.\n",
    "\n",
    "On remarque que c'est le cas à chaque fois pour le split 2 (le dernier split)\n",
    "\n",
    "On peut éventuellement :\n",
    "- Analyser les données qui mènent à cette erreur\n",
    "- Effectuer des splits avec une meilleure stratification que pour le train test split (stratify=True)\n",
    "- Modifier le nombre de cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SiteEnergyUse - Ridge\n",
    "pd.DataFrame(cvs[5].cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SiteEnergyUse - Lasso\n",
    "pd.DataFrame(cvs[6].cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SiteEnergyUse - Elastic\n",
    "pd.DataFrame(cvs[7].cv_results_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch - TotalGHGEmissions\n",
    "\n",
    "Le split 2 reste toujours un peu inférieur au 2 autres splits pour le score r2.\n",
    "\n",
    "La median absolute error est différente et modifie le rang de nos grid search results. Ceci laisse entendre que nous avons parfois des erreurs importantes dans nos prédictions ce qui suppose la présence d'outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itération 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certaines variables n'apporte pas beaucoup à notre modèle. On peut ainsi effectuer une réduction de dimension ce qui aura pour effet d'aider le modèle à apprendre sur des données plus cohérentes et également de réduire le temps d'entrainement.\n",
    "\n",
    "Utilisons également les pipeline !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, MinMaxScaler\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "def encode_use_types(X):\n",
    "    use_types_gfa_df = pd.DataFrame()\n",
    "    # use type columns types/GFA\n",
    "    use_types = [x for x in X.columns if \"GFA\" not in x]\n",
    "    use_types_gfa = [x for x in X.columns if \"GFA\" in x]\n",
    "\n",
    "    for (i,line), gfa in zip(enumerate(X[use_types].values), X[use_types_gfa].values):\n",
    "        # Clés sans doublons\n",
    "        line_k = np.unique(np.array(list(filter(lambda x: x==x, line))))\n",
    "        # On met les valeurs de gfa pour ces clés\n",
    "        use_types_gfa_df.loc[i,line_k] = gfa[:len(line_k)]\n",
    "\n",
    "\n",
    "    use_types_gfa_df = use_types_gfa_df.fillna(0)\n",
    "    use_types_gfa_df = MinMaxScaler().fit_transform(use_types_gfa_df)\n",
    "\n",
    "    return use_types_gfa_df\n",
    "\n",
    "preprocessing = Pipeline([\n",
    "    (\"encoder\", ColumnTransformer([\n",
    "                    (\"standard_scaler\", StandardScaler(), [\"Latitude\", \"Longitude\", \"PropertyGFAParking\", \"PropertyGFABuilding(s)\"]),\n",
    "                    (\"one_hot_encoded\", OneHotEncoder(sparse_output=False), [\"Parking\", \"YearBuiltRange\", \"Neighborhood\", \"CouncilDistrictCode\", \"ZipCode\", \"BuildingType\"]),\n",
    "                    (\"use_types\", FunctionTransformer(encode_use_types, validate=False), [\"LargestPropertyUseType\", \"LargestPropertyUseTypeGFA\", 'SecondLargestPropertyUseType', 'SecondLargestPropertyUseTypeGFA', 'ThirdLargestPropertyUseType', 'ThirdLargestPropertyUseTypeGFA'])\n",
    "                ], remainder=\"drop\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, ~df.columns.isin(targets)]\n",
    "y = df.loc[:, df.columns.isin(targets)]\n",
    "y_emission = y.loc[:, targets[0]]\n",
    "y_electricity = y.loc[:, targets[1]]\n",
    "\n",
    "preprocessing.fit(X, y_emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = preprocessing.transform(X)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "rfe = RFE(model)\n",
    "rfe.fit(X_encoded, y_emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.columns[rfe.get_support()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"rfe\", RFE(GradientBoostingRegressor()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection.fit(X, y_emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = selection.transform(X)\n",
    "X_encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de rompre l'incohérence entre le validation set et le test set on peut nous même effectuer une stratification cette fois-ci sur une autre variable que l'on considère importante, comme le property use type.\n",
    "\n",
    "Ainsi on aura un peu de chaque type de bâtiments dans nos jeux de données.\n",
    "\n",
    "Je ne peux pas encore effectuer une stratification sur ces données car je n'ai pas assez de property use types pour chaque valeur unique. En effet, parfois il y a une ligne avec un property use type unique. Ainsi je vais devoir effectuer un feature engineering pour rassembler certaines valeurs entre elles. Je me souviens qu'il y a la valeur \"autre\" sur laquelle je peux m'appuyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.LargestPropertyUseType.unique().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_over = (df.LargestPropertyUseType.value_counts() > 5).reset_index()\n",
    "value_over = value_over.loc[value_over.LargestPropertyUseType].set_index(\"index\")\n",
    "value_over.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df.LargestPropertyUseType.isin(value_over.index), \"LargestPropertyUseType\"] = \"Other\"\n",
    "df.LargestPropertyUseType.unique().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df.SecondLargestPropertyUseType.isin(value_over.index), \"SecondLargestPropertyUseType\"] = \"Other\"\n",
    "df.loc[~df.ThirdLargestPropertyUseType.isin(value_over.index), \"ThirdLargestPropertyUseType\"] = \"Other\"\n",
    "(set(df.LargestPropertyUseType) | set(df.SecondLargestPropertyUseType) | set(df.ThirdLargestPropertyUseType)).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df.LargestPropertyUseType.value_counts().sort_values(ascending=False).index\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "sns.countplot(data=df, x=\"LargestPropertyUseType\", order=order)\n",
    "plt.title(\"LargestPropertyUseType count\")\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, ~df.columns.isin(targets)]\n",
    "\n",
    "X_encoded = preprocessing.transform(X)\n",
    "len([x for x in X_encoded.columns if \"use_types__\" in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_emission, test_size=0.3, random_state=random_state, stratify=df.LargestPropertyUseType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_usetypes = df.iloc[X_train.index].LargestPropertyUseType\n",
    "X_test_usetypes = df.iloc[X_test.index].LargestPropertyUseType\n",
    "\n",
    "order = X_train_usetypes.unique()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10,14), sharex=True)\n",
    "sns.countplot(x=X_train_usetypes, ax=axs[0], order=order)\n",
    "axs[0].set_ylabel(\"Train set stratification\")\n",
    "sns.countplot(x=X_test_usetypes, ax=axs[1], order=order)\n",
    "axs[1].set_ylabel(\"Test set stratification\")\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "splits = 3\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "y_emission_discret = pd.cut(y_train, 20, labels=False)\n",
    "\n",
    "folds = []\n",
    "for i, (i_train, i_test) in enumerate(skf.split(df.iloc[X_train.index].LargestPropertyUseType, df.iloc[X_train.index].LargestPropertyUseType)):\n",
    "    folds.append([i_train, i_test])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(splits, 1, sharex=True, sharey=True, figsize=(18,20))\n",
    "for i, ax in enumerate(axs):\n",
    "    property_fold = df.iloc[folds[i][1]][\"LargestPropertyUseType\"]\n",
    "    sns.histplot(x=property_fold, kde=True, ax=ax, label=f'Fold-{i}')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    if i == len(axs) - 1:\n",
    "        ax.set_xlabel(\"PropertyUseTypes\")\n",
    "    ax.legend(frameon=False, handlelength=0)\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model_scores = []\n",
    "mean_model_scores = []\n",
    "for model in tqdm(emission_models):\n",
    "    scores = np.array([])\n",
    "    for i_train, i_test in folds:\n",
    "        Xtr, Xte, ytr, yte = X_train.iloc[i_train], X_train.iloc[i_test], y_train.iloc[i_train], y_train.iloc[i_test]\n",
    "\n",
    "        model.fit(Xtr, ytr)\n",
    "        y_pred = model.predict(Xte)\n",
    "\n",
    "        scores = np.append(scores, r2_score(yte, y_pred))\n",
    "    model_scores.append((type(model).__name__, scores))\n",
    "    mean_model_scores.append(scores.mean())\n",
    "\n",
    "[(name, score) for (name,_), score in zip(model_scores, mean_model_scores)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La stratification a bien été effectuée en fonction du type de bâtiments. Peut être que maintenant nous auront de meilleurs résultats pour notre validation/test sets.\n",
    "\n",
    "Or on remarque que les résultats sont pire qu'avant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "for model in tqdm(emission_models):\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "[(name, score) for (name,_), score in zip(model_scores, test_scores)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a toujours un grand écart entre le validation set et le test test. Les prédictions sur le test set sont étrangement bonnes.\n",
    "\n",
    "Si on enlève le random_state on remarque que pour certains splits notre modèle est très fort lors de la prédiction sur notre test set. Ceci renforce le fait que les outliers faussent nos prédiction.\n",
    "\n",
    "C'était une bonne idée de les conserver pour pouvoir prédire avec des données d'outliers, mais si l'outlier déroute nos modèle alors autant s'en débarasser.\n",
    "\n",
    "Essayons de voir pourquoi nos modèles sont si mauvais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "model = emission_models[4]\n",
    "\n",
    "exp = shap.Explainer(model, feature_names=X_encoded.columns)\n",
    "\n",
    "shap_values = exp.shap_values(X_test)\n",
    "\n",
    "p = shap.plots.force(exp.expected_value[0], shap_values, X_test)\n",
    "\n",
    "HTML(f\"<div style='width:100%;background-color:white'>{p.html()}</div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test,  plot_size=[18,10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut clairement conclure que certaines lignes ont plus de poids pour notre modèle. Essayons de regarder de quelles lignes il s'agit pour en savoir plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = exp.expected_value[0]\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = pd.DataFrame(shap_values, columns=X_encoded.columns)\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_outliers = df.iloc[X_test.index].loc[(shap_values > 500).any(axis=1).values]\n",
    "shap_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[X_test.index].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"PropertyGFABuilding(s)\", y=\"TotalGHGEmissions\", hue=df.index.isin(shap_outliers.index))\n",
    "plt.title(\"PropertyBuildingGFA by TotalGHGEmissions colored by shap outlier\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déjà 2 de nos shaply values sont issues de d'outliers. Les autres sont éparpillées à l'intérieur des autres données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.scatterplot(data=df, x=\"LargestPropertyUseType\", y=\"TotalGHGEmissions\", hue=df.index.isin(shap_outliers.index))\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete_index = df[(df.LargestPropertyUseType == \"Hospital (General Medical & Surgical)\") & (df.TotalGHGEmissions > 1e4)].index\n",
    "to_delete_index = np.concatenate((to_delete_index, df[(df.LargestPropertyUseType == \"Medical Office\") & (df.TotalGHGEmissions > 2e3)].index))\n",
    "to_delete_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes uniquement sur le test set. Voyons voir le training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = shap.Explainer(model, feature_names=X_encoded.columns)\n",
    "\n",
    "shap_values = exp.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train,  plot_size=[18,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_outliers = df.iloc[X_train.index].loc[(shap_values > 1.5e3).any(axis=1)]\n",
    "shap_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les shap_outliers sont au dessus de la moyenne pour PropertyGFABuilding(s), TotalGHGEmissions et SiteEnergyUse(kBtu). Ce n'est pas une coïncidence. On va donc retirer ces outliers pour éviter d'entrainer dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.index.isin(np.concatenate((to_delete_index, shap_outliers.index)))]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline([\n",
    "    (\"encoder\", ColumnTransformer([\n",
    "                    (\"standard_scaler\", StandardScaler(), [\"Latitude\", \"Longitude\", \"PropertyGFAParking\", \"PropertyGFABuilding(s)\"]),\n",
    "                    (\"one_hot_encoded\", OneHotEncoder(sparse_output=False), [\"Parking\", \"YearBuiltRange\", \"Neighborhood\", \"CouncilDistrictCode\", \"ZipCode\", \"BuildingType\"]),\n",
    "                    (\"use_types\", FunctionTransformer(encode_use_types, validate=False), [\"LargestPropertyUseType\", \"LargestPropertyUseTypeGFA\", 'SecondLargestPropertyUseType', 'SecondLargestPropertyUseTypeGFA', 'ThirdLargestPropertyUseType', 'ThirdLargestPropertyUseTypeGFA'])\n",
    "                ], remainder=\"drop\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.loc[:, ~df.columns.isin(targets)]\n",
    "y = df.loc[:, df.columns.isin(targets)]\n",
    "y_emission = y.loc[:, targets[0]]\n",
    "y_electricity = y.loc[:, targets[1]]\n",
    "\n",
    "X_encoded = preprocessing.fit_transform(X, y_emission)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_emission, test_size=0.3, random_state=random_state)\n",
    "\n",
    "splits = 3\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "y_emission_discret = pd.cut(y_train, 50, labels=False)\n",
    "\n",
    "folds = []\n",
    "for i, (i_train, i_test) in enumerate(skf.split(y_emission_discret, y_emission_discret)):\n",
    "    folds.append([i_train, i_test])\n",
    "\n",
    "model_scores = []\n",
    "mean_model_scores = []\n",
    "for model in tqdm(emission_models):\n",
    "    scores = np.array([])\n",
    "    for i_train, i_test in folds:\n",
    "        Xtr, Xte, ytr, yte = X_train.iloc[i_train], X_train.iloc[i_test], y_train.iloc[i_train], y_train.iloc[i_test]\n",
    "\n",
    "        model.fit(Xtr, ytr)\n",
    "        y_pred = model.predict(Xte)\n",
    "\n",
    "        scores = np.append(scores, r2_score(yte, y_pred))\n",
    "    model_scores.append((type(model).__name__, scores))\n",
    "    mean_model_scores.append(scores.mean())\n",
    "\n",
    "[(name, score) for (name,_), score in zip(model_scores, mean_model_scores)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont vraiment horribles. Revenont sur nos pas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itération 3\n",
    "\n",
    "On a appris de l'itération 2 que :\n",
    "\n",
    "- Une stratification par le PropertyUseType n'est pas une bonne solution\n",
    "- On a en effet la présence d'outliers\n",
    "\n",
    "D'après ceux-ci, on peut essayer de :\n",
    "\n",
    "- Filtrer les outliers encore plus qu'avant, si un souhaite qu'un outlier puisse être détecté par notre modèle il faut plus de données qui lui ressemble. On peut également opter pour une data augmentation\n",
    "- Remettre en question l'utilité du PropertyUseType. Il faut certainement créer des groupes d'émetteurs comme \"Gros Emetteur\", \"Moyen Emetteur\", \"Petit Emetteur\", ainsi aboutir à une réduction de dimensionalité\n",
    "\n",
    "Filtrons d'abord les outliers et observons s'il y a des changements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons les grosses erreurs qui sont effectuées sur le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline([\n",
    "    (\"encoder\", ColumnTransformer([\n",
    "                    (\"standard_scaler\", StandardScaler(), [\"Latitude\", \"Longitude\", \"PropertyGFAParking\", \"PropertyGFABuilding(s)\"]),\n",
    "                    (\"one_hot_encoded\", OneHotEncoder(sparse_output=False), [\"Parking\", \"YearBuiltRange\", \"Neighborhood\", \"CouncilDistrictCode\", \"ZipCode\", \"BuildingType\"]),\n",
    "                    (\"use_types\", FunctionTransformer(encode_use_types, validate=False), [\"LargestPropertyUseType\", \"LargestPropertyUseTypeGFA\", 'SecondLargestPropertyUseType', 'SecondLargestPropertyUseTypeGFA', 'ThirdLargestPropertyUseType', 'ThirdLargestPropertyUseTypeGFA'])\n",
    "                ], remainder=\"drop\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, ~df.columns.isin(targets)]\n",
    "y = df.loc[:, df.columns.isin(targets)]\n",
    "y_emission = y.loc[:, targets[0]]\n",
    "y_electricity = y.loc[:, targets[1]]\n",
    "\n",
    "X_encoded = preprocessing.fit_transform(X, y_emission)\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_emission = train_test_split(X_encoded, y_emission, test_size=0.3, random_state=random_state)\n",
    "\n",
    "model_metrics, emission_models = compare_models(tts_emission, cv=3)\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = emission_models[4]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tts[0], tts[2])\n",
    "\n",
    "y_pred = model.predict(X_encoded.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_emission, abs(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_emission, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = median_absolute_error(y_emission, y_pred)\n",
    "mae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons les pires prédictions (supérieur à la mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(y_pred) < 0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_mae_i = (y_pred > mae).nonzero()\n",
    "over_mae_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[(np.array(y_pred) < 0).nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e297204aacfc4597d7d358135a658faf9a53ab85f698c95c4e6ba1d206a216bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
